{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction au Deep Learning avec Theano "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " - Julien Guillaumin \n",
    " - Etudiant à Télécom Bretagne en année de césure \n",
    " - Stage chez Continental (Toulouse) - Deep Learning (octobre 2016 à mars 2017)\n",
    " - Kaggle-addict et MOOC-friendly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Objectifs de l'atelier :\n",
    "- découvrir le Deep Learning et les différences avec le Machine Learning \"non profond\"\n",
    "- l'appliquer à la classification d'images [détection d'anomalie]\n",
    "- le tout en découvrant `thenao`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Rappels de Machine Learning \n",
    "2. Découvrir Theano\n",
    "3. Régression logistique pour la classification \n",
    "4. Réseaux de neurones, un premier pas dans le Deep Learning \n",
    "5. Réseaux convolutifs, de 1990 à 2012 \n",
    "6. [La détection d'anomalies par entrainement non-supérvisé !]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I/ Rappels de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Champ d'étude de l'intelligence artificielle \n",
    "- Concevoir des algorithmes qui adaptent leurs réponses en fonction de l'environnement/comportement dans lequel ils se trouvent \n",
    "- **Etape d'apprentissage** : on montre des exemples de comportements -> nos **données**\n",
    "    * l'agorithme doit **ajuster ses paramètres** pour produire la réponse voulue, **minimiser une erreur**\n",
    "    * **Généralisation** et non un apprentissage par coeur \n",
    "- **Etape d'inference** : on met notre algorithme dans un nouvel environnement où il doit produire des réponses correctes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Apprentissage supervisé vs Apprentissage non-supervisé \n",
    "\n",
    "* **Apprentissage supervisé : on connaît la bonne réponse !** \n",
    "  - Algorithmes de classification : estimer la probabilté que cette données appartienne à telle classe\n",
    "  - Algorithmes de régression : prédire une valeur réelle à partir des données \n",
    "\n",
    "\n",
    "* **Apprentissage non-supervisé : trouver une structure/schéma dans les données !**\n",
    "    - Algorithme de Clustering (segmentation de clients, analyse de l'ADN ou de données spatiales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Aujourd'hui : \n",
    "- beaucoup d'apprentissage supervisé pour la classification d'images \n",
    "- un peu de non supervisé pour de la détection d'anomalie \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Apprentissage supervisé \n",
    "Exemples de données : \n",
    "\n",
    "<img src=\"resources/imagenet.jpeg\" width=\"900\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Le i-ème exemple :   $( x^{(i)}, y^{(i)} )$\n",
    "* Les paramètres entraînables du système :   $ \\theta $\n",
    "* Fonction de prédiction (la sortie de notre algorithme) :   $ f_{\\theta}(x^{(i)})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On parle aussi de fonction de coût, de perte.\n",
    "\n",
    "Erreur pour un exemple : $$ E( x^{(i)} , y^{(i)} , \\theta ) = L( f_{\\theta}( x^{(i)} ) ,y^{(i)}) $$\n",
    "\n",
    "Erreur pour un ensemble de données : $$ E( X, Y, \\theta) = \\frac{1}{N}\\sum_{n=1}^N E( x^{(i)} , y^{(i)} , \\theta ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pour la régression : \n",
    " * $f_{\\theta}(x^{(i)}) = \\hat y$ est une approximation de $y^{(i)}$\n",
    " * on peut utiliser l'_Erreur Moyenne Quadratique_ (**MSE**) comme mesure de l'erreur ! \n",
    "$$ L( f_{\\theta}( x^{(i)} ) ,y^{(i)}) = \\frac{1}{2}|| f_{\\theta}(x^{(i)}) - y^{(i)} ||^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pour la classification : \n",
    "* $f_{\\theta}(x^{(i)}) $ est une estimation de $\\Bbb P( \\ Y \\ = \\ i \\ | \\ x^{(i) \\ } , \\theta)$ pour toute classe $i$ (vecteur de probabilités)\n",
    "* **-** log-vraisemblance (Negative Log Likelihood): \n",
    "\n",
    "$$  L( \\ f_{\\theta}( x^{(i)}) \\ , \\ y^{(i)} \\ ) = - \\ log \\ ( P( \\ Y \\ = \\ y^{(i)} \\ | \\ x^{(i) \\ } , \\theta) ) $$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nous sommes revenus à un problème d'optimisation : \n",
    " * **Trouver $\\theta$ qui minimise $E( X, Y, \\theta)$**\n",
    "\n",
    "<img src=\"resources/gradient.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Descente du gradient** :\n",
    "- Algorithme itératif \n",
    "- $ \\lambda $ pas d'apprentissage \n",
    "- Mettre à jour $\\theta$ de façon à se rapprocher du minimum global\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} \\ - \\ \\lambda \\ \\frac{\\partial E(X,Y,\\theta)}{\\partial \\theta}$$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# II/ Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"resources/theano_logo.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Librairie Python pour définir et évaluer des expressions mathématiques \n",
    "* Représentation d'une expression par un graphe \n",
    "* Générer et compiler du code optimisé pour CPU (C/C++) ou GPU (CUDA)\n",
    "* CPU/GPU transparent pour l'utilisateur\n",
    "* Dérivation automatique (très utile pour le gradient)\n",
    "* Syntaxe proche de Numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Définir une expression \n",
    "Un exemple d'expression mathématique : $$c = \\sqrt{a^2 + b^2}$$\n",
    "\n",
    "3 variables symboliques: $$a, b , c$$\n",
    "(scalaires, vecteurs, matrices, tenseur 3D, 4D, ...) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 670MX (CNMeM is enabled with initial size: 10.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T \n",
    "# on y retrouve toutes les variables symboliques et de nombreuses opérations. \n",
    "\n",
    "x = T.matrix('x')\n",
    "y = T.matrix('y')\n",
    "\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"resources/applyAdditionMatrix.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Réprésentation sous forme d'un graphe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"resources/symbolic_graph_unopt.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Compiler l'expression et l'utiliser comme une fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.,   6.],\n",
       "       [ -2.,  13.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compilation de l'expression 'c' sous forme d'une fonction 'f'\n",
    "#le code généré puis compilé est gardé en mémoire \n",
    "f = theano.function([x,y],z)\n",
    "\n",
    "#On peut directement appeler la function 'f'\n",
    "f( [[0,1],[2,8]] , [[2,5],[-4,5]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"resources/symbolic_graph_opt.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimisation \n",
    "- génération de code optimisé pour les opérations et variables de l'expression \n",
    "- CPU ou GPU (indifférent pour l'utilisateur)\n",
    "- optimisation dans la représentation de l'expression !\n",
    "\n",
    "```export THEANO_FLAGS='cuda.root=/usr/local/cuda-7.5,device=gpu,floatX=float32'```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* GPU : x 10 sur ma machine\n",
    "* CUDA, cuDNN, cuBLAS, CNMeM ... \n",
    "\n",
    "![alt text](resources/nvidia.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dérivation dans Thenao\n",
    "\n",
    "- Dérivation automatique d'une expression \n",
    "- Application de la règle de dérivation des fonctions composées ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ y(x) = x^2$$\n",
    "$$ \\ $$\n",
    "$$ \\frac{dy}{dx} = 2x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = T.scalar('x')\n",
    "y = x**2\n",
    "# T.grad( expression scalaire, variable )\n",
    "g_y = T.grad(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"resources/g_y_unopt.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(8.0, dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = theano.function([x], g_y)\n",
    "f(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"resources/g_y_opt.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$f(x)=k \\ x^n$$\n",
    "$$f'(x)=k \\ n \\ x^{n-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " * On passe au premier notebook sur Theano (avec un exemple de régression linéaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Réseaux de Neurones, le début du Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Deep Learning :**\n",
    "* On applique plusieurs non-linéarités à $x^{(i)}$ dans  $ f_{\\theta}(x^{(i)})$\n",
    "* On distingue des couches de non-linéarités\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "** Profondeur d'une architecture : **\n",
    "* nombre de couches où des non-linéarités sont appliquées "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Avantages des architectures profondes :\n",
    "* inspiration du système visuel animal\n",
    "* extraction de caractéristiques à plusieurs niveaux d'abstarction (pratique)\n",
    "* représentation efficace de fonctions complexes (théorique)\n",
    "\n",
    "Inconvénients : \n",
    "* l'entrainement devient vite très long\n",
    "\n",
    "Tâches : classification, régression, réduction de dimensionalité, segmentation d'images, génération d'images, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Perceptron Multicouche \n",
    "* Réseau de neurones artificiels (dès 1980)\n",
    "* Facilité pour empiler les couches \n",
    "\n",
    "<img src=\"resources/mlp1.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"resources/mlp2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Apprentissage : Rétropropagation du gradient \n",
    "\n",
    "\n",
    "- Initilisation des poids du réseau (règle de \"Xavier\" par exemple)\n",
    "- Pour un exemple donné, on effectue la propagation avant\n",
    "- Calcul de l'erreur en sortie (par rapport à une valeur cible)\n",
    "- Calcul du signal d'erreur sur les couches cachées par rétropropagation du gradient \n",
    "- Correction des poids \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Difficultés d'entraîner des modèles profonds \n",
    "* Le nombre de paramètres croît très vite (...Bon dimensionnement du réseau) \n",
    "* Mauvaise diffusion du gradient vers les couches basses (...Bonne initialisation)\n",
    "* Fonction à minimiser fortement non-convexe (... Techniques de régularisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Réseaux convolutifs \n",
    "\n",
    "* Tirer profit de la forte corrélation entre pixels voisins, pour limiter le nombre de paramètres\n",
    "<img src=\"resources/sparse_1D_nn.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Partager des poids, pour limiter le nombre de paramètres \n",
    "<img src=\"resources/conv_1D_nn.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On retrouve tout ça dans la convolution : \n",
    "* On déplace sur l'image un noyau de convolution dont les poids sont les paramètres entraînables\n",
    "<img src=\"resources/kernel_convolution.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On crée ainsi des <b>Feature Maps<b>\n",
    "<img src=\"resources/Conv-Nets.png\" width=\"800\">\n",
    "\n",
    "Ici 4 noyaux 3D, pour passer d'une couche avec 3 feature maps à une couche avec 4 feature maps\n",
    "<br>(3x3x3) x 4 -> 108 paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Pooling (max ou moyenne) \n",
    "* Fonction d'activation à chaque pixel des feature maps\n",
    "\n",
    "<img src=\"resources/pooling.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"resources/Conv_full_model.png\" width=\"1400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"resources/mylenet.png\" width=\"1400\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
