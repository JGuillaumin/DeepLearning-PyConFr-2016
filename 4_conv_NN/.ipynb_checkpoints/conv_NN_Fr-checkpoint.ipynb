{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: unknown error)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano \n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (6, 4)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting et régularisation \n",
    "\n",
    "Overfitting = surapprentissage \n",
    "\n",
    "Le modèle apprend \"par coeur\" les données d'entrainement. \n",
    "Il n'y a plus de généralisation. \n",
    "\n",
    "![Softmax function](../resources/overfitting_s.png )\n",
    "\n",
    "\n",
    "Une technique pour résoudre ce problème : empécher les poids/paramètres d'être trop importants ! \n",
    "\n",
    "On ajoute un terme de régularisation à la fonction de coût : \n",
    "\n",
    "$$ E(X,Y,\\theta) + \\mu_{1} | \\theta | + \\mu_{2} || \\theta ||^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Si W_1 et W_2 sont des variables correspondants aux paramètres du modèle \n",
    "L1 = abs(W_1).sum() + abs(W_2).sum() \n",
    "L2 = (W_1**2).sum() + (W_2**2).sum() \n",
    "\n",
    "cost = cost + mu_1*L1 + mu_2*L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseaux de neurones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif de cette partie construire un réseau de neurones pour classiffier des caractères manuscrits (Digit-Recognizer de Kaggle)\n",
    "\n",
    "* nombre de classes (sortie du réseau) : 10\n",
    "* entrée du réseau : 28x28 = 784 \n",
    "* deux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Données et paramètres d'apprentissage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Données d'entrainement, de validation et de test (labels inconnus)\n",
    "X_train = np.load('../data/digit-recognizer/X_train.npy')\n",
    "X_valid = np.load('../data/digit-recognizer/X_valid.npy')\n",
    "X_test = np.load('../data/digit-recognizer/X_test.npy')\n",
    "\n",
    "## Les labels \n",
    "Y_train = np.load('../data/digit-recognizer/y_train.npy')\n",
    "Y_valid = np.load('../data/digit-recognizer/y_valid.npy')\n",
    "\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)\n",
    "print(Y_train.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_shared =  T.cast(theano.shared(X_train, borrow=True),'float32')\n",
    "Y_train_shared = T.cast(theano.shared(Y_train, borrow=True),'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "n_epochs=100\n",
    "batch_size=128\n",
    "\n",
    "n_train_batches = X_train.shape[0] // batch_size\n",
    "\n",
    "mu_1 = 0.0\n",
    "mu_2 = 0.5\n",
    "\n",
    "# nb de pixels\n",
    "n_in = 28*28\n",
    "\n",
    "#nb de classes\n",
    "n_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 4096)\n",
      "(12500, 4096)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "train = np.load('train.npy').reshape(25000,64*64)/255.0\n",
    "test = np.load('test.npy').reshape(12500,64*64)/255.0\n",
    "label = np.load('label.npy')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.1\n",
    "n_epochs=15\n",
    "batch_size=32\n",
    "L1_reg=0.00\n",
    "L2_reg=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_set_x =  theano.shared(np.asarray(train[:2000],dtype=theano.config.floatX), borrow=True)\n",
    "valid_set_y = T.cast(theano.shared(np.asarray(label[:2000],dtype=theano.config.floatX), borrow=True),'int32')\n",
    "\n",
    "train_set_x = theano.shared(np.asarray(train[2000:],dtype=theano.config.floatX), borrow=True)\n",
    "train_set_y = T.cast(theano.shared(np.asarray(label[2000:],dtype=theano.config.floatX), borrow=True),'int32')\n",
    "\n",
    "test_set_x = theano.shared(np.asarray(test,dtype=theano.config.floatX), borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4096)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set_x.get_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "n_train_batches = train_set_x.get_value().shape[0] // batch_size\n",
    "n_valid_batches = valid_set_x.get_value().shape[0] // batch_size\n",
    "\n",
    "print(n_train_batches)\n",
    "print(n_valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# allocate symbolic variables for the data\n",
    "index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "# generate symbolic variables for input (x and y represent a\n",
    "# minibatch)\n",
    "x = T.matrix('x')  # data, presented as rasterized images\n",
    "y = T.ivector('y')  # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "nkerns=[20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape matrix of rasterized images of shape (batch_size, 28 * 28)\n",
    "# to a 4D tensor, compatible with our LeNetConvPoolLayer\n",
    "# (28, 28) is the size of MNIST images.\n",
    "layer0_input = x.reshape((batch_size, 1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_shape_layer1=(batch_size, 1, 64, 64)\n",
    "filter_shape_layer1=(nkerns[0], 1, 5, 5)\n",
    "poolsize_layer1=(2, 2)\n",
    "\n",
    "\n",
    "fan_in_layer1 = np.prod(filter_shape_layer1[1:])\n",
    "fan_out_layer1 = (filter_shape_layer1[0] * np.prod(filter_shape_layer1[2:]) // np.prod(poolsize_layer1))\n",
    "\n",
    "\n",
    "W_bound_layer1 = np.sqrt(6. / (fan_in_layer1 + fan_out_layer1))\n",
    "W_layer1 = theano.shared(\n",
    "            np.asarray(\n",
    "                rng.uniform(low=-W_bound_layer1, high=W_bound_layer1, size=filter_shape_layer1),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "b_layer1 = theano.shared(value=np.zeros((filter_shape_layer1[0],), dtype=theano.config.floatX), borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_out_layer1 = conv2d(\n",
    "            input=layer0_input,\n",
    "            filters=W_layer1,\n",
    "            filter_shape=filter_shape_layer1,\n",
    "            input_shape=image_shape_layer1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pooled_out_layer1 = pool.pool_2d(\n",
    "            input=conv_out_layer1,\n",
    "            ds=poolsize_layer1,\n",
    "            ignore_border=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_layer1 = T.nnet.relu(pooled_out_layer1 + b_layer1.dimshuffle('x', 0, 'x', 'x'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_shape_layer2=(batch_size, nkerns[0], 30, 30)\n",
    "filter_shape_layer2=(nkerns[1], nkerns[0], 5, 5)\n",
    "poolsize_layer2=(2, 2)\n",
    "\n",
    "\n",
    "fan_in_layer2 = np.prod(filter_shape_layer2[1:])\n",
    "fan_out_layer2 = (filter_shape_layer2[0] * np.prod(filter_shape_layer2[2:]) // np.prod(poolsize_layer2))\n",
    "\n",
    "\n",
    "W_bound_layer2 = np.sqrt(6. / (fan_in_layer2 + fan_out_layer2))\n",
    "W_layer2 = theano.shared(\n",
    "            np.asarray(\n",
    "                rng.uniform(low=-W_bound_layer2, high=W_bound_layer2, size=filter_shape_layer2),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "b_layer2 = theano.shared(value=np.zeros((filter_shape_layer2[0],), dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "\n",
    "\n",
    "conv_out_layer2 = conv2d(\n",
    "            input=output_layer1,\n",
    "            filters=W_layer2,\n",
    "            filter_shape=filter_shape_layer2,\n",
    "            input_shape=image_shape_layer2\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "pooled_out_layer2 = pool.pool_2d(\n",
    "            input=conv_out_layer2,\n",
    "            ds=poolsize_layer2,\n",
    "            ignore_border=True\n",
    "        )\n",
    "\n",
    "\n",
    "output_layer2 = T.nnet.relu(pooled_out_layer2 + b_layer2.dimshuffle('x', 0, 'x', 'x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer3 = output_layer2.flatten(2)\n",
    "\n",
    "n_in = nkerns[1] * 13 * 13\n",
    "n_out = 500\n",
    "\n",
    "W_layer3 = theano.shared(value=np.asarray(\n",
    "                                    rng.uniform( low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                                                 high=np.sqrt(6. / (n_in + n_out)),\n",
    "                                                size=(n_in, n_out)),\n",
    "                                    dtype=theano.config.floatX),\n",
    "                    name='W_layer3', borrow=True)\n",
    "\n",
    "        \n",
    "b_layer3 = theano.shared(value=np.zeros((n_out,), dtype=theano.config.floatX), name='b_layer3', borrow=True)\n",
    "\n",
    "output_layer3 = T.nnet.relu(T.dot(input_layer3, W_layer3) + b_layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_logReg = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_out, 1),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W_logReg',\n",
    "            borrow=True\n",
    "        )\n",
    "b_logReg = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (1,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b_logReg',\n",
    "            borrow=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L1 = abs(W_layer1).sum() + abs(W_layer2).sum() + abs(W_layer3).sum() + abs(W_logReg).sum() \n",
    "L2 = (W_layer1**2).sum() + (W_layer2**2).sum() + (W_layer3**2).sum() + (W_logReg**2).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_y_given_x = T.nnet.sigmoid(T.dot(output_layer3, W_logReg) + b_logReg)\n",
    "\n",
    "cost = T.mean(T.nnet.binary_crossentropy(p_y_given_x, y))\n",
    "\n",
    "y_pred = T.argmax(p_y_given_x, axis=1)\n",
    "\n",
    "errors = T.mean(T.neq(y_pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=errors,\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [W_layer1, b_layer1, W_layer2, b_layer2, W_layer3, b_layer3, W_logReg, b_logReg]\n",
    "\n",
    "gparams = [T.grad(cost, param) for param in params]\n",
    "\n",
    "updates = [\n",
    "        (param, param - learning_rate * gparam)\n",
    "        for param, gparam in zip(params, gparams)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patience = 500 # look as this many examples regardless\n",
    "validation_frequency = min(n_train_batches, patience)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "test_score = 0.\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input dimension mis-match. (input[1].shape[1] = 32, input[2].shape[1] = 1)\nApply node that caused the error: Elemwise{Composite{((i0 * i1 * i2) + (i3 * i4 * i5))}}(TensorConstant{(1, 1) of -1.0}, Elemwise{Cast{int32}}.0, HostFromGpu.0, TensorConstant{(1, 1) of -1.0}, Elemwise{sub,no_inplace}.0, HostFromGpu.0)\nToposort index: 89\nInputs types: [TensorType(float64, (True, True)), TensorType(int32, row), TensorType(float32, matrix), TensorType(float64, (True, True)), TensorType(float64, row), TensorType(float32, matrix)]\nInputs shapes: [(1, 1), (1, 32), (32, 1), (1, 1), (1, 32), (32, 1)]\nInputs strides: [(8, 8), (128, 4), (4, 4), (8, 8), (256, 8), (4, 4)]\nInputs values: [array([[-1.]]), 'not shown', 'not shown', array([[-1.]]), 'not shown', 'not shown']\nOutputs clients: [[Sum{acc_dtype=float64}(Elemwise{Composite{((i0 * i1 * i2) + (i3 * i4 * i5))}}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input dimension mis-match. (input[1].shape[1] = 32, input[2].shape[1] = 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-406ff6699ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtnrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mminibatch_avg_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;31m# iteration number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    872\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/theano/gof/link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input dimension mis-match. (input[1].shape[1] = 32, input[2].shape[1] = 1)\nApply node that caused the error: Elemwise{Composite{((i0 * i1 * i2) + (i3 * i4 * i5))}}(TensorConstant{(1, 1) of -1.0}, Elemwise{Cast{int32}}.0, HostFromGpu.0, TensorConstant{(1, 1) of -1.0}, Elemwise{sub,no_inplace}.0, HostFromGpu.0)\nToposort index: 89\nInputs types: [TensorType(float64, (True, True)), TensorType(int32, row), TensorType(float32, matrix), TensorType(float64, (True, True)), TensorType(float64, row), TensorType(float32, matrix)]\nInputs shapes: [(1, 1), (1, 32), (32, 1), (1, 1), (1, 32), (32, 1)]\nInputs strides: [(8, 8), (128, 4), (4, 4), (8, 8), (256, 8), (4, 4)]\nInputs values: [array([[-1.]]), 'not shown', 'not shown', array([[-1.]]), 'not shown', 'not shown']\nOutputs clients: [[Sum{acc_dtype=float64}(Elemwise{Composite{((i0 * i1 * i2) + (i3 * i4 * i5))}}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "for epoch in tnrange(n_epochs, desc='epochs'):\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "        minibatch_avg_cost = train_model(minibatch_index)\n",
    "            \n",
    "            # iteration number\n",
    "        iter = (epoch) * n_train_batches + minibatch_index\n",
    "\n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "            validation_losses = [validate_model(i)\n",
    "                                     for i in range(n_valid_batches)]\n",
    "            this_validation_loss = np.mean(validation_losses)\n",
    "\n",
    "            print('epoch %i, minibatch %i/%i, validation error %f %%' %(epoch, minibatch_index + 1, n_train_batches,\n",
    "                        this_validation_loss ))\n",
    "\n",
    "            # if we got the best validation score until now\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                best_validation_loss = this_validation_loss\n",
    "                    # test it on \n",
    "end_time = timeit.default_timer()\n",
    "print(( 'Optimization complete with best validation score of %f %%')\n",
    "        % (best_validation_loss * 100.))\n",
    "    \n",
    "print('The code run for %d epochs, with %f epochs/sec' % (epoch, 1. * epoch / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {
    "e42ddb787c0248ea85070c4b377374b2": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
